{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bb8292-f8e1-4c91-8f7e-de9fed2fb8ee",
   "metadata": {},
   "source": [
    "## Understanding ROMP configurations and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bdea05-66a0-48f8-9754-af2735251456",
   "metadata": {},
   "source": [
    "All package parameters are set in `params/config.in` by default. Users need to make sure they are properly set for their project before running the package - that's the first thing to do after installation.   \n",
    "\n",
    "Once we get quite familiar with ROMP, there is an option to make an edited copy of the config file outside of the sitepackage, with user defiend  filename such as `config_project_name.in` (not recommended for new users)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7dfa6-d272-44fa-9817-e75b7d5e3076",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Workflow Structure & Dimensions  \n",
    "\n",
    "This section initializes the project metadata and defines the global dimensions of the evaluation. It serves as the registry for which forecast models will be included in the current run.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **project_name** | `String` | A descriptive title for your experiment. This is often used in plot titles and log files. |\n",
    "| **layout** | `Tuple list` | Defines the internal workflow structure for benchmarking. Standardized as `(\"model\", \"verification_window\")`. |\n",
    "| **model_list** | `Tuple list` | The list of forecast models to be processed (e.g., `\"IFS\"`, `\"NGCM\"`). |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "##### 1. The Importance of \"Forecast Categories\"\n",
    "When defining your `model_list`, all models must belong to the same **forecast category**. \n",
    "* **Example**: Do not mix deterministic models (single output) with probabilistic/ensemble models (multiple outputs) in the same run, as the verification metrics (like MAE vs. RPSS) differ significantly.\n",
    "\n",
    "##### 2. Layout & Dimensions\n",
    "The `layout` parameter is \"frozen\" for major versions of the ROMP package. It tells the system how to organize the resulting multidimensional arrays (tensors). \n",
    "* **Model**: The identifier for the forecast source.\n",
    "* **Verification Window**: The specific time slice (e.g., Day 1-3, Week 1) being evaluated.\n",
    "\n",
    "\n",
    "\n",
    "#### Rules for Tuples in Python Configs\n",
    "The system uses Python's `tuple` syntax for many parameters. Note these two critical formatting rules:\n",
    "\n",
    "1.  **Trailing Commas**: If you are only evaluating **one** model, you must keep a trailing comma so Python recognizes it as a tuple:\n",
    "    * `model_list = (\"IFS\",)` $\\leftarrow$ Correct\n",
    "    * `model_list = (\"IFS\")` $\\leftarrow$ **Incorrect** (Python will treat this as a simple string)\n",
    "2.  **Consistency**: The number of entries in `model_list` must exactly match the number of entries in the subsequent `_list` parameters (like `model_dir_list` and `model_var_list`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f427b-e6dd-4ba8-9769-e20c104e2e4b",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Observation Data Settings  \n",
    "\n",
    "This section defines how the system locates, reads, and interprets the \"Ground Truth\" or observational data.   \n",
    "\n",
    "#### Parameter Breakdown\n",
    "\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **obs** | `String` | Unique identifier for the dataset (e.g., \"IMD\", \"CHIRPS\"). |\n",
    "| **obs_dir** | `Path` | Root directory for observational NetCDF files. |\n",
    "| **obs_file_pattern** | `Tuple` | File naming pattern. `{}` is a placeholder for date/year. |\n",
    "| **obs_var** | `String` | The specific variable name for rainfall inside the NetCDF file. |\n",
    "| **obs_unit_cvt** | `Float/None` | Multiplier for unit conversion (e.g., 1000 to convert m to mm). |\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Implementation Notes & Limitations\n",
    "\n",
    "* **Variable Names**: `obs_var` must exactly match the variable name in your NetCDF file metadata.\n",
    "* **Placeholders**: The `{}` in `obs_file_pattern` is dynamically filled by the processing script. If your files are named by year (e.g., `1998.nc`), the pattern should be `(\"{}.nc\",)`.\n",
    "* **Unit Conversion**: Set `obs_unit_cvt` to `None` if your data is already in the target units (usually mm). Use a float like `1000.0` if the source data is in meters.  \n",
    "\n",
    "#### Verification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da36c2a-e2b9-4aff-8323-e811d4d581f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: Could not find data/obs/2024.nc. Check your 'obs_dir' or 'obs_file_pattern'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Example values from your config\n",
    "obs = \"IMD\"\n",
    "obs_dir = \"data/obs\"\n",
    "obs_file_pattern = (\"{}.nc\", \"data_{}.nc\")\n",
    "\n",
    "# Simulation of file look-up for the year 2024\n",
    "example_year = \"2024\"\n",
    "potential_file = obs_file_pattern[0].format(example_year)\n",
    "full_path = os.path.join(obs_dir, potential_file)\n",
    "\n",
    "if os.path.exists(full_path):\n",
    "    print(f\"✅ Success: Found file at {full_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error: Could not find {full_path}. Check your 'obs_dir' or 'obs_file_pattern'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6186d-4b36-4faf-aa1e-104ddaee7a80",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "---  \n",
    "\n",
    "### 3. Reference Model (Benchmark) Settings\n",
    "\n",
    "This section explains how to set a benchmark (like a historical average or a simpler model) to evaluate your main model's performance.  \n",
    "\n",
    "The reference model provides a baseline for evaluating the performance of your target model. A common practice is to use **Climatology** (long-term historical averages) as the benchmark.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **ref_model** | `String` | The identifier for your benchmark. Use `\"climatology\"` or a specific model name. |\n",
    "| **ref_model_dir** | `Path` | Directory where the reference NetCDF files are located. |\n",
    "| **ref_model_file_pattern** | `Tuple` | Naming convention for reference files. Uses `{}` for date/year placeholders. |\n",
    "| **ref_model_var** | `String` | The rainfall variable name within the reference NetCDF file. |\n",
    "| **ref_model_unit_cvt** | `Float/None` | Scaling factor for reference data (e.g., use `1000` if the reference is in meters). |\n",
    "\n",
    "---\n",
    "\n",
    "#### Why use a Reference Model?\n",
    "\n",
    "The benchmark allows you to calculate \"Skill Scores.\" Instead of just knowing if your model is accurate, you learn if it is *better* than a simple baseline.\n",
    "\n",
    "* **Climatology**: If `ref_model = \"climatology\"`, the system typically compares results against the long-term mean of the observation data.\n",
    "* **Persistence**: You might use a previous day/year as a reference to see if the model adds value beyond simply repeating the past.\n",
    "\n",
    "\n",
    "\n",
    "#### Configuration Tips\n",
    "\n",
    "1.  **Shared Directories**: It is common for `ref_model_dir` to be the same as `obs_dir` if you are using historical observational data as your climatology baseline.\n",
    "2.  **Consistency**: Ensure `ref_model_var` and `ref_model_unit_cvt` result in the same physical units as your observation and forecast data. If the units don't match, the error metrics (like RMSE or Bias) will be mathematically invalid.\n",
    "3.  **File Pattern**: Like the observation section, the `{}` placeholder ensures the script can iterate through multiple years of benchmark data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220117d3-f947-4110-903c-e21af0a957fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 4. Forecast Model Settings  \n",
    "\n",
    "This section is crucial as it handles the \"Forecast Models\" you are evaluating. Unlike the observation and reference sections, these settings are formatted as Lists of Tuples because the pipeline can process multiple models (e.g., IFS and NGCM) simultaneously for comparison.  \n",
    "\n",
    "This section allows you to define one or more forecast models for evaluation. Because you may want to compare multiple models at once, these parameters are defined as **Tuple lists**.  \n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **model_dir_list** | `Tuple` | Directories for each forecast model. Order must match your model list. |\n",
    "| **model_var_list** | `Tuple` | The rainfall variable name in each model's NetCDF file (e.g., `\"tp\"` for Total Precipitation). |\n",
    "| **unit_cvt_list** | `Tuple` | Conversion factors for each model. Use `None` if units are already in mm. |\n",
    "| **file_pattern_list** | `Tuple` | File naming patterns for each model. Uses `{}` for time-based placeholders. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Handling Multiple Models - understanding the mapping\n",
    "\n",
    "The power of this section lies in its **parallel structure**. Each item in the first list corresponds to the same index in the other lists. The system matches these parameters by their **index** (their position in the tuple).  \n",
    "\n",
    "**Example Scenario:**\n",
    "* **Model 1 (IFS):** Uses variable `tp`, stored in `data/ifs`.\n",
    "* **Model 2 (NGCM):** Also uses variable `tp`, stored in `data/ngcm`.\n",
    "\n",
    "**Example Mapping:**\n",
    "* **Index 0**: `model_dir_list[0]` $\\rightarrow$ `model_var_list[0]` $\\rightarrow$ `unit_cvt_list[0]`\n",
    "* **Index 1**: `model_dir_list[1]` $\\rightarrow$ `model_var_list[1]` $\\rightarrow$ `unit_cvt_list[1]`\n",
    "\n",
    "\n",
    "#### Limitations & Potential Errors\n",
    "\n",
    "1.  **Index Mismatch**: If `model_dir_list` has 3 paths but `model_var_list` only has 2 names, the notebook will crash with an `IndexError`. **Always ensure all lists have the same length.**\n",
    "2.  **Variable Variations**: Note that while the variable name is often the same (e.g., `\"tp\"`), different centers (ECMWF vs. NCEP) might use different default units.\n",
    "    * *Tip:* Check if one model is in **meters** (requires `unit_cvt = 1000`) and the other is already in **mm** (requires `None`).\n",
    "3.  **File Naming**: If Model A uses `YYYY.nc` but Model B uses `model_YYYY.nc`, you must reflect this in the `file_pattern_list` using `(\"{}.nc\", \"model_{}.nc\")`.\n",
    "4.  **Immutability**: Since these are defined as **Tuples** (using parentheses `()`), they cannot be changed globally once the script starts running. If you need to add a model dynamically during a session, you would need to redefine the entire tuple.\n",
    "\n",
    "#### Unit Consistency Check\n",
    "A common mistake in forecast evaluation is comparing \"Precipitation Rate\" (mm/day, kg/m2/s) with \"Total Accumulation\" (mm). Check your metadata to ensure the `unit_cvt_list` brings all models to a consistent **mm/day** or **mm/period** basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7c6ed-6d2d-4724-af6b-53cc374acdfb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 5. Region & Threshold Definitions  \n",
    "\n",
    "This section defines the spatial boundaries for your analysis. The tool provides three independent methods—**Region**, **Shapefile**, and **Polygon Masking**—to subset or mask your data.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **region** | `String` | Required. Defines a target area (e.g., \"Ethipoia\"). This typically pulls predefined coordinates from a separate settings file. |\n",
    "| **shpfile_dir** | `Path` | Specifies a directory containing shapefiles to define the boundary. |\n",
    "| **polygon** | `Boolean` | If `True`, the system applies a precise polygon-based mask using definitions found in `params/region_def`. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Three Methods of Spatial Definition\n",
    "\n",
    "The package uses these parameters to determine which grid points to include in the evaluation:\n",
    "\n",
    "1.  **Region-Based**: Uses a named geographic area. This is often the simplest method, relying on hardcoded latitude/longitude bounds for well-known areas.\n",
    "2.  **Shapefile-Based**: Uses the physical `.shp` files located in `shpfile_dir`. This allows you to use external GIS data to define the study area.\n",
    "3.  **Polygon Masking**: When `polygon = True`, the system moves beyond a rectangular \"bounding box\" and masks the data to the exact edges of the region polygons. This ensures that only data points strictly inside the boundary are calculated.\n",
    "\n",
    "\n",
    "\n",
    "#### Configuration Rules & Limitations\n",
    "\n",
    "* **Independence**: These methods can be used independently depending on your project needs. \n",
    "* **Polygon Requirements**: If `polygon` is set to `True`, you **must** have the corresponding definitions set up in `params/region_def`.\n",
    "* **Data Alignment**: Ensure your NetCDF grid resolution is sufficient for your chosen boundary. If your region is very small (e.g., a small district) and your model resolution is coarse (e.g., 1 degree), you may end up with very few data points for analysis.\n",
    "* **Pathing**: Double-check that `shpfile_dir` points to a folder containing all the required shapefile components (`.shp`, `.shx`, `.dbf`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f9676-ba0f-4fdc-ad7d-950df7c3f27f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 6. Rainy season/Monsoon Onset Criteria (Physics Logic)\n",
    "\n",
    "This section defines the meteorological \"rules\" that determine the onset of the rainy season or monsoon. The logic typically involves identifying a transition from a dry state to a sustained wet state, while ensuring that \"false starts\" (isolated rain events) are filtered out.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **wet_init** | `Float` | The minimum rain (mm) required to trigger the \"potential\" start of the onset. |\n",
    "| **wet_threshold** | `Float` | The required accumulated rainfall (mm) over a period to confirm onset. |\n",
    "| **wet_spell** | `Integer` | The minimum number of days the wet conditions must persist. |\n",
    "| **dry_threshold** | `Float` | The maximum rainfall (mm) allowed to still consider a day \"dry.\" |\n",
    "| **dry_spell** | `Integer` | The maximum allowed consecutive dry days immediately following a potential onset. |\n",
    "| **dry_extent** | `Integer` | The search window (days) to look for a dry spell. Set to `0` to disable the dry-spell check. |\n",
    "| **thresh_file** | `Path/None` | Path to a NetCDF containing spatially varying thresholds. If `None`, `wet_threshold` is used globally. |\n",
    "| **thresh_var** | `String` | The variable name for the threshold inside the `thresh_file`. |\n",
    "| **onset_percentage_threshold** | `Float` | For ensembles: The fraction of members (0.0–1.0) that must agree on onset for it to be official. |\n",
    "\n",
    "---\n",
    "\n",
    "#### The Onset Logic Flow\n",
    "\n",
    "The algorithm generally follows these steps to identify the onset date:\n",
    "1. **Initial Trigger**: Finds the first day where rainfall exceeds `wet_init`.\n",
    "2. **Accumulation Check**: Ensures the total rain over the period reaches `wet_threshold`.\n",
    "3. **Persistence (Wet Spell)**: Checks if the wet conditions last for at least `wet_spell` days.\n",
    "4. **False-Start Filtering (Dry Spell)**: If `dry_extent` is greater than 0, the search looks ahead. If a dry spell longer than `dry_spell` occurs within the `dry_extent` window, the initial trigger is rejected as a \"false onset.\"\n",
    "\n",
    "\n",
    "\n",
    "#### Spatial vs. Scalar Thresholds\n",
    "\n",
    "Precipitation varies wildly by geography (e.g., the Western Ghats vs. Rajasthan). \n",
    "* **Scalar**: Using `wet_threshold = 10` applies the same 10mm rule to every grid point.\n",
    "* **Spatial**: By providing a `thresh_file` (a netcdf file with 2-D lat/lon rainfall threshold field), the system uses a unique threshold for every pixel based on historical climatology (`thresh_var`). This is much more scientifically robust for large regions like India.\n",
    "\n",
    "#### Working with Ensembles\n",
    "If you are using probabilistic models (ensembles), `onset_percentage_threshold` is key. \n",
    "* If set to `0.5`, the onset date is only recorded for a grid point when **50% or more** of the ensemble members agree that the onset criteria have been met.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eed1c8-8b01-47f5-89b2-53d30e99ff01",
   "metadata": {},
   "source": [
    "<br>\n",
    "#### Application Example: Translating Scientific Definitions to Config Settings\n",
    "\n",
    "To better understand Section 6, let's translate a common scientific definition of monsoon onset into the specific parameters used by this package.\n",
    "\n",
    "##### The Objective\n",
    "> \"Identify the first wet day (**≥1 mm**) of the first **5-day** wet sequence from **April 1st 2025** that receives at least the **climatological onset amount** in April–October without being followed by a **10-day dry spell** (receiving **less than 5 mm**) in the following **30 days**.\"\n",
    "\n",
    "##### The Configuration Translation\n",
    "\n",
    "To implement the logic above, your `config.in` (and the related Section 7 dates) should be set as follows:\n",
    "\n",
    "| Scientific Requirement | Config Parameter | Setting |\n",
    "| :--- | :--- | :--- |\n",
    "| First wet day (≥1 mm) | `wet_init` | `1` |\n",
    "| 5-day wet sequence | `wet_spell` | `5` |\n",
    "| Climatological amount | `thresh_file` | `\"data/climatology_thresholds.nc\"` |\n",
    "| Use scalar if no file | `wet_threshold` | *(Used only if thresh_file not provided)* |\n",
    "| 10-day dry spell | `dry_spell` | `10` |\n",
    "| Less than 5 mm | `dry_threshold` | `5` |\n",
    "| Following 30 days | `dry_extent` | `30` |\n",
    "| April 1st start | `start_date` | `(2025, 4, 1)` |\n",
    "\n",
    "---\n",
    "Note for `start_date` setting, see section 7. \n",
    "\n",
    "##### Visualizing the Logic Flow\n",
    "\n",
    "\n",
    "\n",
    "1.  **Search Start**: The algorithm begins scanning from your `start_date` (April 1st).\n",
    "2.  **The Trigger**: It looks for a day where rain $\\ge$ `wet_init` (1 mm).\n",
    "3.  **Sustainability**: Once triggered, it checks the next `wet_spell` (5 days). The total rain in these 5 days must exceed the value found in your `thresh_file`.\n",
    "4.  **The \"False Start\" Check**: After the 5-day wet spell, the algorithm looks ahead for `dry_extent` (30 days). If it finds a sequence of `dry_spell` (10 days) where each day is below `dry_threshold` (5 mm), the onset is **rejected** as a false start, and the search continues.\n",
    "\n",
    "---\n",
    "\n",
    "##### Pro-Tip: The `dry_extent` Requirement\n",
    "A common mistake is setting `dry_extent` too short. For the logic above, `dry_extent` must be at least **30** days. If you set it to `5`, the \"10-day dry spell\" rule will be completely ignored, potentially leading to \"False Onsets\" caused by pre-monsoon thundershowers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194bfe1-632e-4dab-96ad-de5f0bdad8b5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 7. Temporal Settings (Dates & Years)\n",
    "\n",
    "This section manages the time-windows for both your forecast evaluation and your climatological baseline. It also handles the logic for \"Initialization Days,\" which is crucial for sub-seasonal to seasonal (S2S) models that only run on specific days of the week.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **start_date / end_date** | `Tuple` | The full range for evaluation (Year, Month, Day). |\n",
    "| **start_year_clim / end_year_clim** | `Integer` | The year range used to calculate the historical mean. |\n",
    "| **years** | `Tuple` | (Optional) Manually specify which years to analyze, overriding the range. |\n",
    "| **years_clim** | `Tuple` | (Optional) Manually specify years for the climatology baseline. |\n",
    "| **init_days** | `Tuple` | Days of the week the model is initialized (0=Monday, 3=Thursday). |\n",
    "| **date_filter_year** | `Integer` | A reference year used to map calendar dates to specific weekdays. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Climatology vs. Evaluation Period\n",
    "\n",
    "It is vital to distinguish between these two timeframes:\n",
    "1.  **Evaluation Period**: The specific days/years you are benchmarking the model's accuracy (e.g., 2013–2015).\n",
    "2.  **Climatological Period**: The years used to define \"normal\" rainfall. Usually, this is a much longer period (e.g., 30 years), but in the config template, it is matched to the evaluation years.\n",
    "3.  **years** and **years_clim** are optional. By default, a list of consecutive years are created according to **start_date / end_date** and **start_year_clim / end_year_clim**, but can be overiden by list of specific years (not necessary in consecutive order) as in **years** and **years_clim**. Set to `None` if not used in the package. \n",
    "\n",
    "\n",
    "\n",
    "#### The Initialization Filter\n",
    "\n",
    "Most S2S forecast models (like the IFS) do not run every day. They might only run twice a week.\n",
    "* **`init_days = (0, 3)`**: This tells the script to only look for forecast data initialized on **Mondays** and **Thursdays**.\n",
    "* **`date_filter_year`**: Because the date of a \"Monday\" changes every year, the system uses a reference year (e.g., 2024) to lock in the `MM/DD` combinations. For example, if May 6th was a Monday in 2024, the script will look for May 6th in 2013, 2014, and 2015 as an initialization candidate.\n",
    "\n",
    "#### Limitations & Tips\n",
    "\n",
    "* **Leap Years**: The system automatically handles February 29th, but ensure your `start_date` and `end_date` cover the full monsoon season (typically May through October for the Indian Monsoon).\n",
    "* **Trailing Commas**: If your model only initializes on one day (e.g., only Monday), remember the tuple rule: `init_days = (0,)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d549e1c-6de3-4caa-bca3-bd8badce8a46",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 8. Evaluation & Verification Parameters\n",
    "\n",
    "This section defines the \"resolution\" of your evaluation in terms of time (temporal metrics for success.). It determines how far into the future the model is tested and how much \"wiggle room\" (tolerance) is allowed for a forecast to be considered correct.  Since weather forecasts naturally degrade over time, these parameters allow you to analyze model skill at different \"lead times\" (how far in advance the forecast was made).\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **verification_window_list** | `Tuple of Tuples` | Specific ranges of days post-initialization to evaluate (e.g., Week 1 vs. Week 2). |\n",
    "| **tolerance_days_list** | `Tuple` | The \"margin of error\" allowed in a verification window. If the model predicts onset 3 days late, it is \"correct\" if tolerance is $\\ge 3$. |\n",
    "| **max_forecast_day** | `Integer` | The absolute limit of the forecast data (e.g., a 15-day or 30-day forecast). |\n",
    "| **day_bins** | `Tuple of Tuples` | Smaller sub-windows used for grouping probabilistic statistics and plotting. |\n",
    "\n",
    "\n",
    "\n",
    "#### Understanding Verification Windows vs. Bins\n",
    "\n",
    "While they look similar, they serve different purposes:\n",
    "\n",
    "1.  **Verification Windows**: These are the \"Macro\" windows. You might want to see how the model performs in the first two weeks (`1, 15`) versus the second two weeks (`16, 30`). The model's skill is usually much higher in the first window.\n",
    "2.  **Day Bins**: These are the \"Micro\" windows for grouping **probabilistic forecast** statistics. They are used to group daily data into chunks (like 5-day intervals) for metric statistics and plots.\n",
    "\n",
    "\n",
    "#### The Tolerance Factor\n",
    "\n",
    "The `tolerance_days_list` is critical for calculating \"Hit Rates.\" \n",
    "* A tolerance of **0** means the forecast must match the observation exactly.\n",
    "* A tolerance of **3** means if the observed onset is June 5th, any forecast between June 2nd and June 8th is considered a \"Hit.\"\n",
    "\n",
    "#### Constraints & Limitations\n",
    "\n",
    "* **Alignment**: The `max_forecast_day` must not exceed the actual length of your NetCDF forecast files. If your files only contain 15 days of data, setting this to 30 will cause the script to look for non-existent data.\n",
    "* **Overlap**: Ensure your `day_bins` fit within your `max_forecast_day`. For example, if `max_forecast_day` is 15, your bins shouldn't go up to 20.\n",
    "* **Tuple Formatting**: Note the double parentheses for nested ranges: `((start, end), (start, end))`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398a532-b8d7-4b7b-89e4-1f5f80981175",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 9. Metrics Selection\n",
    "\n",
    "This section defines the statistical \"grading system\" for your models. It allows you to toggle specific performance metrics on or off depending on whether you are evaluating a single forecast (Deterministic) or a collection of forecasts (Probabilistic/Ensemble). The flags acts as toggle switches for the various statistical analyses the pipeline will perform. It is divided into two main categories: **Deterministic** (focused on error and accuracy) and **Probabilistic** (focused on uncertainty and reliability).\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **FAR** | `Boolean` | **False Alarm Ratio**: How often the model predicted an onset that didn't happen. |\n",
    "| **MAE** | `Boolean` | **Mean Absolute Error**: The average number of days the forecast was off. |\n",
    "| **MR** | `Boolean` | **Miss Rate**: How often the monsoon started but the model failed to predict it. |\n",
    "| **probabilistic** | `Boolean` | Set to `True` if you are evaluating an **Ensemble** model. |\n",
    "| **members** | `Tuple/Str` | A list of specific IDs for ensemble members to be included in the calculation. |\n",
    "\n",
    "\n",
    "\n",
    "#### Deterministic vs. Probabilistic Evaluation\n",
    "\n",
    "The metrics you choose depend heavily on the nature of your forecast:\n",
    "\n",
    "1.  **Deterministic Metrics (`probabilistic = False`)**:\n",
    "    Best for single-trajectory models (e.g., a single high-resolution run). These metrics measure the direct distance between the forecast date and the observed date.\n",
    "    \n",
    "\n",
    "2.  **Probabilistic Metrics (`probabilistic = True`)**:\n",
    "    Used when you have multiple versions of the same forecast (Ensembles). \n",
    "    * **Brier Score (BS)**: Measures the accuracy of probability forecasts.\n",
    "    * **Ranked Probability Score (RPS)**: Evaluates the entire probability distribution.\n",
    "    * **Reliability**: Generates data for Reliability Diagrams to see if the model is \"overconfident\" or \"underconfident.\"\n",
    "    \n",
    "\n",
    "#### Configuration Logic\n",
    "\n",
    "* **The Member List**: If `probabilistic = True`, the script looks for the specific member IDs listed in `members`. It can take a tuple of integers e.g., `(1, 5, 7, 9)`, a str specifying member ID range e.g., `\"1-5\"`. If set to `None` or `'All'` or `''`, the benchmark takes all members available in the data. \n",
    "* **Skill Scores**: Setting `skill_score = True` typically calculates how much better your forecast is compared to the `ref_model` defined in Section 3.\n",
    "* **Resource Management**: Calculating probabilistic metrics like **AUC** and **Reliability** is computationally expensive. If you are processing a very large geographic region, turning these off can speed up the execution during the testing phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d8f55-b1f6-4363-ade1-e6a8c7298725",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### 10. Output & Graphics Settings\n",
    "\n",
    "This final section of the configuration determines how your results are stored and visualized. It allows you to toggle between raw data exports (NetCDF/CSV) and various scientific visualization styles. This section manages the \"End Products\" of your analysis. You can choose to save raw processed data for further use in GIS software or generate publication-ready figures directly within the pipeline.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **dir_out / dir_fig** | `String` | Directories where data files and images will be saved, respectively. |\n",
    "| **save_fig** | `Boolean` | Master switch to enable or disable all image generation. |\n",
    "| **save_nc_...** | `Boolean` | Exports processed spatial metrics (like MAE or Climatology) as NetCDF files. |\n",
    "| **save_csv_score** | `Boolean` | Exports tabular skill scores, useful for external analysis in Excel or Pandas. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Visualization Options\n",
    "\n",
    "The pipeline offers several specialized meteorological plots. Setting these to `True` will generate them automatically:\n",
    "\n",
    "* **Spatial Maps**: Visualizes errors (MAE, FAR, MR) or Climatology onset dates across the geographic grid.\n",
    "    \n",
    "* **Heatmaps**: Excellent for comparing multiple models or different lead times at a glance. \"Panel\" heatmaps group several metrics into a single figure.\n",
    "    \n",
    "* **Reliability Curves**: Essential for probabilistic forecasts to check if the forecast probability matches the observed frequency.\n",
    "* **Bar Plots**: Best for comparing summary scores like Brier Skill Score (BSS) or Area Under Curve (AUC) across different model versions.\n",
    "\n",
    "#### Best Practices for Output\n",
    "\n",
    "1.  **Storage Space**: Saving spatial metrics as NetCDF (`save_nc_... = True`) can consume significant disk space if you are working with high-resolution global data over many years.\n",
    "2.  **Workflow Efficiency**: During the initial setup or debugging phase, it is often helpful to set `plot_... = False` to speed up the script execution. Once the logic is verified, toggle them to `True` for the final run.\n",
    "3.  **Directory Creation**: Ensure the directories defined in `dir_out` and `dir_fig` exist in your project folder, or that the script has the permissions to create them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55728e-3bfa-438f-a5b8-d0186babcc12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv-momp)",
   "language": "python",
   "name": "momp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
